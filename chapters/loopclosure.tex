% !Mode:: "TeX:UTF-8"
\chapter{Loop Closure}
\begin{mdframed}  
	\textbf{Goal of Study}
	\begin{enumerate}[labelindent=0em,leftmargin=1.5em]
		\item Understand why loop closure is needed in SLAM.
		\item Understand the principles of bag-of-words model. 
		\item Use DBoW3 to detect similar images. 
	\end{enumerate}
\end{mdframed}

In this lecture, we will introduce another main module in SLAM: loopback detection. We know that the primary purpose of SLAM (front-end, back-end) is to estimate camera movement. However, the loop closure module is quite different from the previous content, so it is usually considered as an independent module. We will introduce the loop detection method in visual SLAM: the bag of words model, and carry out an experiment on the DBoW library so that readers can get a more intuitive understanding.
\newpage
\includepdf{resources/other/ch12.pdf}

\newpage
\section{Loop Closure and Detection}
\subsection{Why Loop Closure is Needed}
We have already introduced the front-end and the back-end: the front-end provides short-time trajectory/landmarks estimation and the map's initial value. The back-end is responsible for optimizing all these data. However, if we only consider the adjacent keyframes like VO, then the errors will inevitably accumulate with time so that the entire SLAM will suffer from the accumulative error. The result of long-term estimation will not be reliable. In other words, we cannot construct \textbf{globally consistent} trajectories and maps.

Let's take an example. In the map-building stage of autonomous driving, we usually designate the collection vehicle to circle several times in a given area to cover all the collection areas. Suppose we extract the features at the front end, then ignore the feature points, and use a pose graph to optimize the entire trajectory at the back end, as shown in \autoref{fig:drift}(a)~. Since the frontend gives only partial pose constraints, for example, it may be $\bm{x}_1-\bm{x}_2, \bm{x}_2-\bm{x}_3$, etc. However, due to the error in the estimation of $\bm{x}_1$, $\bm{x}_2$ is determined according to $\bm{x}_1$, $\bm{x}_3$ is again determined by $ \bm{x}_2$. By analogy, errors will be accumulated, making the result of back-end optimization look like what is shown in \autoref{fig:drift}~(b), which gradually tends to be inaccurate. In this application scenario, we should use the loop closure information to determine that the vehicle reaches the same place when we pass the same road.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.8\textwidth]{loopclosure/illustrate-loop.pdf}
	\caption{Accumulated drift. (a) Real trajectory. (b) Accumulated error if we only consider adjacent keyframes. (c) Add loop closure to reduce the accumulated drift. }
	\label{fig:drift}
\end{figure}

Although the backend can estimate the maximum posterior error, it depends on the structure of the BA or the pose graph. If there are only adjacent keyframe constraints, we cannot do much, nor can we eliminate the accumulated error. However, the loop closure detection module can give some constraints on \textbf{longer time} other than adjacent frames: for example, the pose constraint between $\bm{x}_1-\bm{x}_{100}$. Why are there constraints between them? Because we noticed that the camera returns to a previously visited scene, and it acquires similar data ln history. The key to loop detection is how to detect this fact effectively. If we can successfully detect this, we can provide more valid constraints for the backend pose graph to get a better estimate, especially a globally consistent estimate. Since the pose graph can be regarded as a point-spring system, loop detection is equivalent to adding additional springs to the graph, which improves the system's stability. The reader can also intuitively imagine that the loop edge "pulls" the edge with accumulated error back to the correct position-if the loop itself is correct.

Loop detection is of great significance to SLAM systems. It is related to the correctness of our estimated trajectory and map in \textbf{long time}. On the other hand, since loop detection provides the correlation between current data and historical data, we can also use loop detection for \textbf{relocation}. Relocation is also beneficial in most applications. For example, if we record a track for a scene in advance and build a map, then we can let the robot follow this track for navigation, and relocation can help us determine our position on this track. Therefore, the loop detection improves the accuracy and robustness of the entire SLAM system. In some cases, we call a system with only a front-end and a local backend as \textbf{VO} and call a system with loop closing and a global backend as \textbf{SLAM}.

\subsection{How to Close the Loops}
Let's consider how to implement loop detection. There are several different ways of thinking about this problem, including theoretical and engineering.

The simplest way is to perform feature matching on any image pairs and determine which of them are related according to the number of correct matches. This is indeed a simple and effective idea. The disadvantage is that we blindly assume that "any two images may have loops," which makes the number of detection too large: for $N$ possible loops, we have to detect $C_N^2$ times. It has the complexity of $O(N^2)$, which grows too fast as the trajectory becomes longer and is not practical in most real-time systems. Another simple way is to extract historical data and perform loop detection randomly. For example, randomly select five frames among $n$ frames and compare them with the current frame. This approach can maintain a constant time calculation, but when the number of frames $N$ increases, the probability of drawing a loop is significantly reduced, making the detection efficiency low.

The simple ideas mentioned above are too coarse. Although random detection is indeed useful in some implementations like {\cite{Endres2014}}, we at least hope that there is a prediction of "there may be a loop somewhere" so that the detection is not so blind. Such approaches can be roughly divided into two ideas: odometry-based or appearance-based. Based on the geometric relationship, when we find that the current camera moves close to a certain position before, we detect whether they have a loop relationship \cite{Hahnel2003}. This is naturally an intuitive idea, but it is hard to estimate the accumulated drift amount unless we have global position measurements like GPS. Therefore, this approach has a logical problem because the goal of loop detection is to eliminate accumulated errors. But the odometry-based approach assumes that "the accumulated error is small so that the loop can be detected." If the assumption does not hold, such methods cannot work when the cumulative error is large \cite{Beeson2010}.

The other method is based on appearance. It has nothing to do with the estimation of the frontend or the backend and only determines the loop detection relationship based on the two images' similarity. This approach eliminates accumulated errors and makes the loop detection module a relatively independent module in the SLAM system (of course, the frontend can provide the extracted feature points). Since it was proposed in the early 21st century, the appearance-based loop detection method can effectively work in different scenarios and has become the mainstream method in visual SLAM and applied to the actual system {\cite{Ulrich2000, Latif2013, Mur-Artal2015}}.

In the appearance-based loop detection algorithm, the core problem is how to calculate the similarity between images. For example, for image $\bm{A}$ and image $\bm{B}$, we need to design a method to calculate the similarity score between them: $s(\bm{A}, \bm{B })$. Of course, this score will take a value in a certain interval, and when it is greater than a certain amount, we think that there is a loop. Readers may have questions: Is it difficult to calculate the similarity between two images? For example, intuitively, an image can be expressed as a matrix, so how about subtracting two images directly and then taking a certain norm?

\begin{equation}
	s(\bm{A}, \bm{B}) = \| \bm{A}-\bm{B} \|.
\end{equation}

Why don't we do this?

\begin{enumerate}
	\item As mentioned earlier, the pixel grayscale is an unstable measurement value, which is severely affected by the ambient light and camera exposure. Assuming that the camera is not moving and we turn on an electric light, the image will be brighter overall. In this way, even for the same data, we will get a significant difference value.
	\item On the other hand, when the camera's viewing angle changes a little, even if each object's luminosity does not change, their pixels will be transformed in the image, resulting in a large difference in value.
\end{enumerate}

Due to the existence of these two situations, in practice, even for very similar images, $\bm{A}-\bm{B}$ will often get an (unrealistic) enormous value. So we say that this function {cannot reflect the similar relationship between images}. It involves a definition of ``good'' and ``bad.'' We have to ask, what kind of function can better reflect the similar relationship, and what kind of function is not good enough? From here, two concepts can be drawn: \textbf{perceptual aliasing} and \textbf{perceptual variability}. Let's discuss it in more detail now.

\subsection{Precision and Recall}
From a human point of view (at least we think), we can feel the fact that "the two images are similar" or "the two photos were taken from the same place" with high accuracy. But since we have not yet grasped the human brain's working principle, we cannot clearly describe how we accomplish this. From a program point of view, we hope that algorithms can reach judgments consistent with humans or facts. When we feel that the two images were taken from the same place, we expect the loop detection algorithm to give the same result as "this is a loop." Conversely, if we think that the two images were taken from different places, then the program should also give a judgment that "this is not a loop." Readers with a machine learning background should feel how similar this passage is to machine learning.  Of course, the judgment of the algorithm is not always consistent with our human thinking, so there may be four situations in \autoref{table:loopclosure}~:


\begin{table}[!htp]
	\centering
	\caption{Classification of the loop detection results}
	\label{table:loopclosure}
	\begin{tabu}{c|c|c}
		\toprule
		Algorithm $\backslash$ Fact & Is loop & Not loop\\ 
		\midrule
		Is loop & True Positive & False Positive \\ 
		Not loop & False Negative & True Negative\\ 
		\bottomrule
	\end{tabu} 
\end{table}

The term negative/positive come from medical terms. False-positive is also called perceptual bias, and false negative is called perceptual variation (see \autoref{fig:FPandFN}). For the convenience of writing, we use the abbreviation TP for true-positive, and FN for false-negative, etc. Since we want the algorithm to be consistent with human judgment, we hope that TP and TN should be as high as possible, and FP and FN should be as low as possible. Therefore, for a particular algorithm, we can count the number of occurrences of TP, TN, FP, and FN on a certain data set, and calculate two statistics: \textbf{accuracy rate} and \textbf{recall rate} (precision \& recall)
\begin{equation}
	\mathrm{Precision} = \mathrm{TP}/(\mathrm{TP}+\mathrm{FP}), \quad \mathrm{Recall} = \mathrm{TP}/(\mathrm{TP}+\mathrm{FN}).
\end{equation}

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.9\textwidth]{loopclosure/FPandFN}
	\caption{Example of false-positive and false-negative scenes. Left: the images look same but actually taken from different space. Right: the images are from the same place but the appearance is different.}
	\label{fig:FPandFN}
\end{figure}

Literally, the accuracy rate describes the probability that all the loops extracted by the algorithm are indeed true loops. The recall rate refers to the probability of loops being detected from all real loops. Why shall we take these two statistics? Because they are representative and usually a pair of \textbf{contradiction}.

An algorithm often has many setting parameters. For example, when a certain threshold is raised, the algorithm may become more ``strict''. It detects fewer loops and improves accuracy. But at the same time, because the number of detection has decreased, many real loops may be missed, resulting in a decline in the recall rate. Conversely, if we choose a more relaxed configuration, the number of detected loops will increase, resulting in a higher recall rate. But there may be incorrectly detected loops so that the accuracy rate will decrease.

In order to evaluate the quality of the algorithm, we will test its $P$ and $R$ values under various configurations and then make a precision-recall curve (see \autoref{fig:PRCurve}). When using the recall rate on the horizontal axis and the accuracy rate on the vertical axis, we will care about the degree to which the entire curve deviates to the upper right, the recall rate at 100\% accuracy or the accuracy at 50\% recall rate, as evaluation indicators. However, please note that we usually cannot say that algorithm A is better than algorithm B in general. We may say that A has a good recall rate when the accuracy rate is high, while B can guarantee a good accuracy rate when the recall rate is 70\%, and so on.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.66\textwidth]{loopclosure/prcurve}
	\caption{Example of the precision-recall curve {\cite{Gao2015a}}. As the recall rate increases, the detection conditions become looser and the accuracy rate decreases. A good algorithm can still guarantee a better accuracy rate in high recall rate.}
	\label{fig:PRCurve}
\end{figure}

It is worth mentioning that in SLAM, we have higher requirements for accuracy while being relatively tolerant to the recall. The false-positive loops will add fundamentally wrong edges to the backend pose graph, sometimes causing the optimization algorithm to give completely wrong results. Imagine if the SLAM mistakenly treats all the desks as the same one. What will happen to the created map? You may see that the corridor is not straight, the walls are staggered, and finally, the entire map is invalid. In contrast, if the recall rate is lower, some loops are probably not detected, and the map may be affected by some accumulated errors, but the other loops may eliminate them. Therefore, when choosing the loop detection algorithm, we are more inclined to set the parameters more strictly or add the step of \textbf{loop verification} after the detection.

So, back to the previous question, why not use $\bm{A}-\bm{B}$ to calculate similarity? We will find that its accuracy and recall are inferior to most current methods, and there may be many false-positive or false-negative cases, so it is ``not good.'' So, which method is better?

\section{Bag of Words}


