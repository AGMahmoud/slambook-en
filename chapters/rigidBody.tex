% !Mode:: "TeX:UTF-8"
\chapter{3D Rigid Body Motion}

\begin{mdframed}
	\textbf{Goal of Study}
	\begin{enumerate}
		\item Understand the description of rigid body motion in three-dimensional space: rotation matrix, transformation matrix, quaternion and Euler angle.
		\item Understand the matrix and geometry module usage of the Eigen library.
	\end{enumerate}
\end{mdframed}

In the last lecture, we explained the framework and content of visual SLAM. This lecture will introduce one of the basic problems of visual SLAM: \textbf{ How to describe the motion of a rigid body in three-dimensional space?} Intuitively, we certainly know that this consists of one rotation plus one translation. Translation does not really have much problem, but the processing of rotation is a hassle. We will introduce the meaning of rotation matrices, quaternions, Euler angles, and how they are computed and transformed. In the practice section, we will introduce the linear algebra library Eigen. It provides a C++ matrix calculation, and its Geometry module also provides the structure described quaternion like rigid body motion. Eigen's optimization is perfect, but there are some special places to use it, we will leave it to the program.

\section{Rotation Matrix}
\label{sec:3.1}
\subsection{Point, Vector and Coordinate System}
The space in our daily life is three-dimensional, so we are born to be used to the movement of three-dimensional space. The three-dimensional space consists of three axes, so the position of one spatial point can be specified by three coordinates. However, we should now consider \textbf{rigid body} , which has not only its position, but also its own posture. The camera can also be viewed as a rigid body in three dimensions, so the position is where the camera is in space, and the attitude is the orientation of the camera. Combined, we can say, "The camera is in the space $ ( 0, 0 , 0 ) $ point, facing the front". But this natural language is cumbersome, and we prefer to describe it in a mathematical language.

We start with the most basic content: \textbf{points} and \textbf{vectors}. Points are the basic elements in space, no length, no volume. Connecting the two points forms a vector. A vector can be thought of as an arrow pointing from one point to another. We need to remind the reader that, please do not confuse the vector with its \textbf{coordinates}. A vector is one of the things in space, such as $ \mathbf{a}$ . Here $ \mathbf{a} $ does not need to be associated with several real numbers. Only when we specify a \textbf{coordinate system} in this three-dimensional space can we talk about the coordinates of the vector in this coordinate system, that is, find several real numbers corresponding to this vector.

With the knowledge of linear algebra, the coordinates of a point in 3D space can also be described by $ \mathbb{R}^3$. How to describe it? Suppose that in this linear space, we find a set of \textbf{base} \footnote{Just a reminder here, the base is a set of linearly independent vectors in the space, normally being orthognal and has unit-length.} $ (\mathbf{e}_1,\mathbf{e}_2,\mathbf{e}_3) $ , then, the arbitrary vector $ \mathbf{a} $ has a \textbf{coordinate} under this set of bases:

\begin{equation}
\mathbf{a} = \left[ {{\mathbf{e}_1},{\mathbf{e}_2},{\mathbf{e}_3}} \right]\left[ \begin{array}{l}
{a_1}\\
{a_2}\\
{a_3}
\end{array} \right] = {a_1}{\mathbf{e}_1} + {a_2}{\mathbf{e}_2} + {a_3}{\mathbf{e}_3}.
\end{equation}

Here $ (a_ 1 , a_ 2 , a_ 3 )^ \mathrm {T} $ is called $\mathbf {a}$'s coordinates \footnote {We use column vectors in this book which is same as most of the  mathematics books.}. The specific values of the coordinates are related to the vector itself, and also the selection of the bases. In $\mathbb{R}^3$, the coordinate system usually consists of 3 orthogonal coordinate axes (although it can also be non-orthogonal, it is rare in practice). For example, given $ \mathbf {x} $ and $ \mathbf {y} $ axis, the $ \mathbf {z} $ axis can be found using the right-hand (or left-hand) rule by $ \mathbf {x} \times  \mathbf {y} $. According to different definitions, the coordinate system is divided into left-handed and right-handed. The third axis of the left hand system is opposite to the right hand system. Most 3D libraries use right-handed (such as OpenGL, 3D Max, etc.), and some libraries use left-handed (such as Unity, Direct3D, etc.).

Based on basic linear algebra knowledge, we can talk about the operations between vectors/vectors, and vectors/numbers, such as scalar multiplication, vector addition, subtraction, inner product, outer product, and so on. Multiplication, addition and subtraction are fairly basic and intuitive. For example, the result of adding two vectors is to add their respective coordinates, subtraction, and so on. I won't go into details here. Internal and external products may be somewhat unfamiliar to the reader, and their calculations are given here. For $ \mathbf {a}, \mathbf {b} \in  \mathbb {R}^ 3 $ , in the usual sense \footnote {the inner product also has formal rules, but this book only discusses the usual inner product.} , the inner product of $\mathbf{a}, \mathbf{b}$ can be written as:

\begin{equation}
\mathbf{a} \cdot \mathbf{b} = { \mathbf{a}^\mathrm{T}}\mathbf{b} = \sum\limits_{i = 1}^3 {{a_i}{b_i}}  = \left| \mathbf{a} \right|\left| \mathbf{b} \right|\cos \left\langle {\mathbf{a},\mathbf{b}} \right\rangle ,
\end{equation}
where $ \left \langle { \mathbf {a}, \mathbf {b}} \right \rangle $ refers to the angle between the vector $ \mathbf {a}, \mathbf {b} $ . The inner product can also describe the projection relationship between vectors. The outer product is like this:

\begin{equation}
\label{eq:cross}
\mathbf{a} \times \mathbf{b} = \left\| {\begin{array}{*{20}{c}}
	\mathbf{e}_1 & \mathbf{e}_2 & \mathbf{e}_3 \\
	{{a_1}}&{{a_2}}&{{a_3}}\\
	{{b_1}}&{{b_2}}&{{b_3}}
	\end{array}} \right\| = \left[ \begin{array}{l}
{a_2}{b_3} - {a_3}{b_2}\\
{a_3}{b_1} - {a_1}{b_3}\\
{a_1}{b_2} - {a_2}{b_1}
\end{array} \right] = \left[ {\begin{array}{*{20}{c}}
	0&{ - {a_3}}&{{a_2}}\\
	{{a_3}}&0&{-{a_1}}\\  
	{-{a_2}}&{{a_1}}&0  
	\end{array}} \right] \mathbf{b} \buildrel \Delta \over = { \mathbf{a}^ \wedge } \mathbf{b}.
\end{equation}

The result of the outer product is a vector whose direction is perpendicular to the two vectors, and the length is $ \left | \mathbf{a} \right | \left | \mathbf{b} \right | \left \langle { \mathbf {a}, \mathbf {b}} \right \rangle  $, which is also the area of the quadrilateral of the two vectors. For the outer product operations, we introduce the $ ^ \wedge $ operator here, which means writing $ \mathbf{a} $ as a matrix. In fact, it is a \textbf {skew-symmetric matrix}\footnote{Skew-symmetric matrix means $ \mathbf{A} $ satisfies $ \mathbf{A}^ \mathrm{T}=- \mathbf{A}$. }. You can take $ ^ \wedge $ as an skew-symmetric symbol. It turns the outer product $ \mathbf{a} \times  \mathbf{b} $ into the multiplication of the matrix and the vector $ { \mathbf{a}^ \wedge } \mathbf{b} $ , which turns it into a linear operator. This symbol will be used frequently in the following sections, and this symbol is a one-to-one mapping, meaning that for any vector, it corresponds to a unique anti-symmetric matrix, and vice versa:

\begin{equation}
\mathbf{a}^\wedge = \left[ {\begin{array}{*{20}{c}}
	0&{-{a_3}}&{{a_2}}\\  
	{{a_3}}&0&{ - {a_1}}\\
	{ - {a_2}}&{{a_1}}&0
	\end{array}} \right].
\end{equation}

At the same time, note that the vector operations such as addition, subtraction, internal and external products, can be calculated even when we do not have their coordinates. For example, although the inner product can be expressed by the sum of the product products of the two vectors when we know the coordinates, it can also be calculated by the length and the angle even if their coordinates are unknown. Therefore, the inner product result of the two vectors is independent of the selection of the coordinate system.

\subsection{Euclidean Transforms}
We often define a variety of coordinate systems in the real scene. In robotics, you define one coordinate system for each link and joint; in 3D mapping, we also define the coordinate system for each cuboid and cylinder. If we consider a moving robot, it is common practice to set an inertial coordinate system (or world coordinate system) that can be considered stationary, such as the $x_W, y_W, z_W$ defined in Fig.~\ref{fig:axisTransform}. At the same time, the camera or robot is a moving coordinate system, such as the coordinate system defined by $x_C, y_C, z_C$. We might ask: a vector $\mathbf{p}$ in the camera's field of view, has coordinates $\mathbf{p}_c$ in the camera coordinate system; and in the world coordinate system, its coordinates are $ \mathbf{p}_w$, then how is the conversion between these two coordinates? At this time, it is necessary to first obtain the coordinate value of the point for the robot coordinate system, and then according to the robot pose \textbf{transform} into the world coordinate system. We need a mathematical way to describe this transformation. As we will see later, we can describe it with a matrix $\mathbf{T}$.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.7\textwidth]{rigidMotion/axisTransform.pdf}
    \caption {Coordinate transformation. For the same vector $ \mathbf{p}$ , it coordinates in the world $\mathbf{p}_W$ and coordinates in the camera system $ \mathbf{p}_C$ is different. This transformation relationship is described by the transform matrix $ \mathbf{T} $ . }
    \label{fig:axisTransform}
\end{figure}

Intuitively, the motion between two coordinate systems consists of a rotation plus a translation, which is called \textbf {rigid body motion}. Obviously, the camera movement is a rigid body one. During the rigid body motion, the length and angle of the a vector will not change. Imagine you throw your phone into the air and \footnote {Please don't put it into practice because it will fall on the ground and crash.}, there may only be differences in spatial position and posture, and its own length, angle of each face, etc. will not change. The phone will not be squashed like an eraser or be stretched during this motion. At this point, we say that the phone's motion is a \textbf {Euclidean Transform}.

The Euclidean transform consists of rotation and translation. Let's first consider about the rotation. We have an unit-length orthogonal base $ ( \mathbf {e}_ 1 , \mathbf {e}_ 2 , \mathbf {e}_ 3 ) $. After a rotation it becomes $ ( \mathbf {e}_ 1 ' , \mathbf {e}_ 2 ', \mathbf {e}_ 3 ') $ . Then, for the same vector $ \mathbf {a} $ (the vector does not move with the rotation of the coordinate system), its coordinates in two coordinate systems are $ [a_ 1 , a_ 2 , a_ 3 ] ^ \mathrm {T} $ and $[a'_ 1 , a'_ 2 , a'_ 3 ]^ \mathrm {T} $ . Because the vector itself has not changed, according to the definition of coordinates, there are:

\begin{equation}
\left[ \mathbf{e}_1,\mathbf{e}_2,\mathbf{e}_3 \right]\left[ \begin{array}{l}
{a_1}\\
{a_2}\\
{a_3}
\end{array} \right] = \left[ \mathbf{e}_1', \mathbf{e}_2', \mathbf{e}_3' \right]\left[ \begin{array}{l}
a'_1\\
a'_2\\
a'_3
\end{array} \right].
\end{equation}

To describe the relationship between the two coordinates, we multiply the left and right sides of the above equation by $ \left [ \begin {array}{l}
\mathbf{e}_1^\mathrm{T}\\
\mathbf{e}_2^\mathrm{T}\\
\mathbf{e}_3^\mathrm{T}
\end {array} \right ] $ , then the coefficient on the left becomes the identity matrix, so:

\begin{equation}
\left[ \begin{array}{l}
{a_1}\\
{a_2}\\
{a_3}
\end{array}\right]=\left[{\begin{array}{*{20}{c}}    
    {\mathbf{e}_1^\mathrm{T}\mathbf{e}_1'} & {\mathbf{e}_1^\mathrm{T}\mathbf{e}_2'} & {\mathbf{e}_1^\mathrm{T}\mathbf{e}_3'}\\
    {\mathbf{e}_2^\mathrm{T}\mathbf{e}_1'} & {\mathbf{e}_2^\mathrm{T}\mathbf{e}_2'} & {\mathbf{e}_2^\mathrm{T}\mathbf{e}_3'}\\
    {\mathbf{e}_3^\mathrm{T}\mathbf{e}_1'} & {\mathbf{e}_3^\mathrm{T}\mathbf{e}_2'} & {\mathbf{e}_3^\mathrm{T}\mathbf{e}_3'}
    \end{array}} \right]\left[ \begin{array}{l}
a_1'\\
a_2'\\
a_3'
\end{array} \right] \buildrel \Delta \over = \mathbf{R} \mathbf{a}'.
\end{equation}
We take the intermediate matrix out and define it as a matrix $ \mathbf{R} $ . This matrix consists of the inner product between the two sets of bases, describing the coordinate transformation relationship of the same vector before and after the rotation. As long as the rotation is the same, this matrix is the same. It can be said that the matrix $ \mathbf{R} $ describes the rotation itself. So we call it the \textbf{rotation matrix}. At the same time, the components of the matrix are the inner product of the two coordinate system bases. Since the length of the base vector is 1, it is actually the cosine of the angle between the base vectors. So this matrix is also called \textbf{Direction Cosine Matrix}. We will call it the rotation matrix in the following.

The rotation matrix has some special properties. In fact, it is an orthogonal matrix with a determinant of 1 \footnote{Orthogonal matrix is a matrix whose inverse is its transpose. The orthogonality of the rotation matrix can be derived directly from the definition. } \footnote{The determinant is 1 is artificially defined. In fact, its determinant is $\pm 1 $, but the rotation with determinant $ - 1 $ is called improper rotation, that is, one rotation plus one reflection. }. Conversely, an orthogonal matrix with a determinant of 1 is also a rotation matrix. So, you can define a collection of $n$ dimensional rotation matrices as follows:
\begin{equation}
\mathrm{SO}(n) = \{ \mathbf{R} \in \mathbb{R}^{n \times n} | \mathbf{R R}^\mathrm{T} = \mathbf{I}, \mathrm{det} (\mathbf{R})=1 \}.
\end{equation}

$\mathrm{SO}(n) $ is the meaning of \textbf {Special Orthogonal Group}. We leave the contents of the ``group'' to the next lecture. This collection consists of a rotation matrix of $ n $ dimensional space, in particular, $\mathrm {SO}(3)$ refers to the rotation of the three-dimensional space. By this way, we can talk directly about the rotation transformation between the two coordinate systems without having to start from the bases.

Since the rotation matrix is an orthogonal matrix, its inverse (ie, transpose) describes an opposite rotation. According to the above definition, there are:
\begin{equation}
\mathbf{a} '= \mathbf{R}^{-1} \mathbf{a} = \mathbf{R}^ \mathrm{T} \mathbf{a}.
\end{equation}
Obviously $ \mathbf{R}^\mathrm{T} $ portrays an opposite rotation.

In the Euclidean transformation, there is translation in addition to rotation. Consider the vector $ \mathbf{a} $ in the world coordinate system , after a rotation (depicted by $ \mathbf{R} $) and a translation of $ \mathbf{t} $ , you get $ \mathbf{a}' $ , then put the rotation and translation together, there are:
\begin{equation}
\label{eq:RT}
\mathbf{a} '= \mathbf{R} \mathbf{a} + \mathbf{t}.
\end{equation}
Where $ \mathbf{t} $ is called a translation vector. Compared to rotation, the translation part simply adds the translation vector to the coordinates after the rotation, which is very simple. By the above formula, we completely describe the coordinate transformation relationship of an Euclidean space using a rotation matrix $ \mathbf{R} $ and a translation vector $ \mathbf{t}$. In practice, we will define the coordinate system 1, coordinate system 2, then the vector $ \mathbf{a} $ under the two coordinates is $ \mathbf{a}_1 , \mathbf{a}_2 $ , they are The relationship between the two, in accordance with the complete writing, should be:
\begin{equation}
\mathbf{a}_1 = \mathbf{R}_{12} \mathbf{a}_2 + \mathbf{t}_{12}.
\end{equation}
Here $ \mathbf{R}_{12} $ means ``rotation the vector from coordinate system 2 to coordinate system 1''. Since the vector is multiplied to the right of this matrix, its subscript is \textbf{read from right to left}. This is just a customary way of writing this book. Coordinate transformations are easy to confuse, especially if multiple coordinate systems exist. Similarly, if we want to express ``rotation matrix from 1 to 2'', we write it as $ \mathbf{R}_{21}$. The reader must be clear about the notation here, because different books have different notations, some will be recorded as the top left/subscript, and the text will be written on the right side.

About $\mathbf{t}_{12}$ , it actually corresponds a vector from the coordinate system 1 origin pointing to the coordinate system 2 origin, whose \textbf{coordinates are taken under coordinate system 1}, so I suggest readers to put it as ``a vector from 1 to 2''. But the reverse $ \mathbf{t}_{21} $ , which is a vector from 2's origin to 1's origin, whose \textbf{coordinates are taken in coordinate system 2}, is not equal to $-\mathbf{t}_{12}$, but is also related to the rotation of the two systems\footnote{Although from the vector level, they are indeed inverse relations, but the coordinates of the two vectors are not opposite. Can you find out why it looks like this? }. Therefore, when beginners ask the question ``Where is my coordinates?'', we need to clearly explain the meaning of this sentence. Here ``my coordinates'' normally refers to the vector from the world system pointing to the origin of the robot system, and then take the coordinates in the world's base. Corresponding to the mathematical symbol, it should be the value of $ \mathbf{t}_{WC} $ . For the same reason, it is not $ - \mathbf {t}_{CW}$, but actually $-\mathbf{R}_{CW}^T \mathbf{t}_{CW}$.

\subsection{Transform Matrix and Homogeneous Coordinates}
The formula \eqref{eq:RT} fully expresses the rotation and translation of Euclidean space, but there is still a small problem: the transformation relationship here is not a linear relationship. Suppose we made two transformations: $ \mathbf {R}_ 1 , \mathbf {t}_ 1 $ and $ \mathbf {R}_ 2 , \mathbf {t}_ 2 $:

\[
\mathbf{b} = {\mathbf{R}_1} \mathbf{a} + {\mathbf{t}_1}, \quad \mathbf{c} = {\mathbf{R}_2} \mathbf{b} + {\mathbf{t}_2}.
\]
So, the transformation from $ \mathbf{a} $ to $ \mathbf{c} $ is:
\[
\mathbf{c} = {\mathbf{R}_2}\left( {{\mathbf{R}_1} \mathbf{a} + {\mathbf{t}_1}} \right) + {\mathbf{t}_2}.
\]
This form is not elegant after multiple transformations. Therefore, we introduce homogeneous coordinates and transformation matrices, rewriting the form \eqref{eq:RT}:
\begin{equation}
\left[\begin{array}{l} 
\mathbf {a} ' \\
1
\end{array} \right] = 
\left[ {\begin{array}{*{20}{c}}
    \mathbf{R}&\mathbf{t}\\
    {{\mathbf{0}^\mathrm{T}}}&1
    \end{array}} \right]
\left[ \begin{array}{l}
\mathbf {a} \\
1
\end{array} \right]  \buildrel \Delta \over = \mathbf{T} \left[ \begin{array}{l}
\mathbf {a} \\
1
\end{array} \right].
\end{equation}

This is a mathematical trick: we add $ 1 $ at the end of a 3D vector and turn it into a 4D vector called \textbf{homogeneous coordinates}. For this four-dimensional vector, we can write the rotation and translation in one matrix, making the whole relationship a linear relationship. In this formula, the matrix $ \mathbf {T} $ is called \textbf{Transform Matrix}.

We temporarily use $  \tilde { \mathbf {a} } $ to represent the homogeneous coordinates of $ \mathbf {a} $. Then, relying on homogeneous coordinates and transformation matrices, the superposition of the two transformations can have a good form:
\begin{equation}
\tilde{\mathbf{b}} = \mathbf{T}_1 \tilde{\mathbf{a}}, \  \tilde{\mathbf{c}} = \mathbf{T}_2 \tilde{\mathbf{b}} \quad \Rightarrow \tilde{\mathbf{c}} = \mathbf{T}_2 \mathbf{T_1} \tilde{\mathbf{a}}.
\end{equation}
But the symbols that distinguish between homogeneous and non-homogeneous coordinates make us annoyed, because here we only need to add 1 at the end of the vector or remove 1 to make it a normal one\footnote {But the purpose of the homogeneous coordinates is not limited to this, we will come back to it in Chapter 7.}. So, without ambiguity, we will write it directly as $ \mathbf {b}= \mathbf {T} \mathbf {a} $ , and by default we just assume a homogeneous coordinate conversion is made if needed\footnote { Note that if homogeneous coordinate transformation is not performed, the matrix multiplication here does not make sense. }.

Regarding the transformation matrix $ \mathbf{T} $, it has a special structure: the upper left corner is the rotation matrix, the right side is the translation vector, the lower left corner is $ \mathbf{0} $ vector, and the lower right corner is 1. This matrix is also known as the Special Euclidean Group:

\begin{equation}
\mathrm{SE}(3) = \left\{ \mathbf{T} = \left[ {\begin{array}{*{20}{c}}
    \mathbf{R} & \mathbf{t} \\
    {{\mathbf{0}^\mathrm{T}}} & 1
    \end{array}} \right]
\in \mathbb{R}^{4 \times 4} | \mathbf{R} \in \mathrm{SO}(3), \mathbf{t} \in \mathbb{R}^3\right\} .
\end{equation}

Like $ \mathrm{SO}( 3 ) $ , solving the inverse of the matrix represents an inverse transformation:

\begin{equation}
{ \mathbf{T}^{ - 1}} = \left[ {\begin{array}{*{20}{c}}
    {{\mathbf{R}^\mathrm{T}}}&{ - {\mathbf{R}^\mathrm{T}}\mathbf{t}}\\
    {{\mathbf{0}^\mathrm{T}}}&1
    \end{array}} \right].
\end{equation}

Again, we use the notation of $ \mathbf{T}_{12} $ to represent a transformation from 2 to 1. Moreover, in order to keep the symbol concise, in the case of no ambiguity, the symbols of the homogeneous coordinates and the ordinary coordinates are not deliberately distinguished in the later sections. For example, when we write $ \mathbf {T} \mathbf{a} $ , we use homogeneous coordinates (otherwise we can't calculate). When you write $ \mathbf{Ra} $ , you use non-homogeneous coordinates. If they are written in the same equation, it is assumed that the conversion from homogeneous coordinates to normal coordinates is already done - because the conversion between homogeneous and non-homogeneous coordinates is actually very easy. In C++ programs. You can do this with \textbf{operator overloading} to ensure that the operations you see in the program are correct.

Let's take a review now. First, we introduce the vector and its coordinate representation, and introduce the operation between the vectors; then, the motion between the coordinate systems is described by the Euclidean transformation, which consists of translation and rotation. The rotation can be described by the rotation matrix $ \mathrm{SO}( 3 ) $ , while the translation is directly described by a $ \mathbb{R}^ 3 $ vector. Finally, if the translation and rotation are placed in a matrix, the transformation matrix $ \mathrm{SE}( 3 ) $ is formed .

\section{Practice: Using Eigen}
The practical part of this lecture has two sections. In the first part, we will explain how to use Eigen to represent matrices and vectors, and then extend to the calculation of rotation matrix and transformation matrix. The code for this section is in \textbf{slambook2/ch3/useEigen}.

Eigen\footnote{Official home page: \url{http://eigen.tuxfamily.org/index.php?title=Main_Page}. } is a C++ open source linear algebra library. It provides fast linear algebra operations on matrices, as well as functions such as solving equations. Many upper-level software libraries also use Eigen for matrix operations, including g2o, Sophus, and others. In the theoretical part of this lecture, let's learn about Eigen's programming.

Eigen may not be installed on your PC. Please enter the following command to install it:

\begin{lstlisting}[language=sh,caption=Terminal input:]
sudo apt-get install libeigen3-dev
\end{lstlisting}

Most of the commonly used libraries in our book are available in the Ubuntu software source. Later, if you want to install a library, you may want to search for the Ubuntu software source. With the apt command, we can easily install Eigen. Looking back at the previous lesson, we know that a library consists of header files and library files. The default location of the Eigen header file should be in "/usr/include/eigen3/". If you are not sure, you can find it by entering the following command:

\begin{lstlisting}[language=sh,caption=Terminal input:]
sudo locate eigen3
\end{lstlisting}

Compared to other libraries, Eigen is special in that it is a library built with pure header files (this is amazing!). This means you can only find its header files, not binary files like .so or .a. When you use it, you only need to import Eigen's header file, you don't need to link the library file (because it doesn't have a library file). Write a piece of code below to actually practice the use of Eigen:
\begin{lstlisting}[language=c++,caption=slambook2/ch3/useEigen/eigenMatrix.cpp]
#include <iostream>
using namespace std;

#include <ctime>
// Eigen core
#include <Eigen/Core>
// Algebraic operations of dense matrices (inverse, eigenvalues, etc.)
#include <Eigen/Dense>
using namespace Eigen;

#define MATRIX_SIZE 50

/****************************
* This program demonstrates the use of the basic Eigen type
****************************/

int main(int argc, char **argv) {
    // All vectors and matrices in Eigen are Eigen::Matrix, which is a template
    // class. Its first three parameters are: data type, row, column Declare a 2*3
    // float matrix
    Matrix<float, 2, 3> matrix_23;
    
    // At the same time, Eigen provides many built-in types via typedef, but the
    // bottom layer is still Eigen::Matrix For example, Vector3d is essentially
    // Eigen::Matrix<double, 3, 1>, which is a three-dimensional vector.
    Vector3d v_3d;
    // This is the same
    Matrix<float, 3, 1> vd_3d;
    
    // Matrix3d is essentially Eigen::Matrix<double, 3, 3>
    Matrix3d matrix_33 = Matrix3d::Zero(); // initialized to zero
    // If you are not sure about the size of the matrix, you can use a matrix of
    // dynamic size
    Matrix<double, Dynamic, Dynamic> matrix_dynamic;
    // simpler
    MatrixXd matrix_x;
    // There are still many types of this, we doesn't list them one by one.
    
    // Here is the operation of the Eigen array
    // input data (initialization)
    matrix_23 << 1, 2, 3, 4, 5, 6;
    // output
    cout << "matrix 2x3 from 1 to 6: \n" << matrix_23 << endl;
    
    // Use () to access elements in the matrix
    cout << "print matrix 2x3: " << endl;
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 3; j++)
        cout << matrix_23(i, j) << "\t";
        cout << endl;
    }
    
    // The matrix and vector are multiplied (actually still matrices and matrices)
    v_3d << 3, 2, 1;
    vd_3d << 4, 5, 6;
    
    // But in Eigen you can't mix two different types of matrices, like this is
    // wrong Matrix<double, 2, 1> result_wrong_type = matrix_23 * v_3d; should be
    // explicitly converted
    Matrix<double, 2, 1> result = matrix_23.cast<double>() * v_3d;
    cout << "[1,2,3;4,5,6]*[3,2,1]=" << result.transpose() << endl;
    
    Matrix<float, 2, 1> result2 = matrix_23 * vd_3d;
    cout << "[1,2,3;4,5,6]*[4,5,6]: " << result2.transpose() << endl;
    
    // Also you can't misjudge the dimensions of the matrix
    // Try canceling the comments below to see what Eigen will report.
    // Eigen::Matrix<double, 2, 3> result_wrong_dimension =
    // matrix_23.cast<double>() * v_3d;
    
    // some matrix operations
    // Four operations are not demonstrated, just use +-*/.
    Matrix_33 = Matrix3d::Random(); // Random Number Matrix
    cout << "random matrix: \n" << matrix_33 << endl;
    cout << "transpose: \n" << matrix_33.transpose() << endl;
    cout << "sum: " << matrix_33.sum() << endl;
    cout << "trace: " << matrix_33.trace() << endl;
    cout << "times 10: \n" << 10 * matrix_33 << endl;
    cout << "inverse: \n" << matrix_33.inverse() << endl;
    cout << "det: " << matrix_33.determinant() << endl;
    
    // Eigenvalues
    // Real symmetric matrix can guarantee successful diagonalization
    SelfAdjointEigenSolver<Matrix3d> eigen_solver(matrix_33.transpose() *
    matrix_33);
    cout << "Eigen values = \n" << eigen_solver.eigenvalues() << endl;
    cout << "Eigen vectors = \n" << eigen_solver.eigenvectors() << endl;
    
    // Solving equations
    // We solve the equation of matrix_NN * x = v_Nd
    // The size of N is defined in the previous macro, which is generated by a
    // random number Direct inversion is the most direct, but the amount of
    // inverse operations is large.
    
    Matrix<double, MATRIX_SIZE, MATRIX_SIZE> matrix_NN =
    MatrixXd::Random(MATRIX_SIZE, MATRIX_SIZE);
    matrix_NN =
    matrix_NN * matrix_NN.transpose(); // Guarantee semi-positive definite
    Matrix<double, MATRIX_SIZE, 1> v_Nd = MatrixXd::Random(MATRIX_SIZE, 1);
    
    Clock_t time_stt = clock(); // timing
    // Direct inversion
    Matrix<double, MATRIX_SIZE, 1> x = matrix_NN.inverse() * v_Nd;
    cout << "time of normal inverse is "
    << 1000 * (clock() - time_stt) / (double)CLOCKS_PER_SEC << "ms" << endl;
    cout << "x = " << x.transpose() << endl;
    
    // Usually solved by matrix decomposition, such as QR decomposition, the speed
    // will be much faster
    time_stt = clock();
    x = matrix_NN.colPivHouseholderQr().solve(v_Nd);
    cout << "time of Qr decomposition is "
    << 1000 * (clock() - time_stt) / (double)CLOCKS_PER_SEC << "ms" << endl;
    cout << "x = " << x.transpose() << endl;
    
    // For positive definite matrices, you can also use cholesky decomposition to
    // solve equations.
    time_stt = clock();
    x = matrix_NN.ldlt().solve(v_Nd);
    cout << "time of ldlt decomposition is "
    << 1000 * (clock() - time_stt) / (double)CLOCKS_PER_SEC << "ms" << endl;
    cout << "x = " << x.transpose() << endl;
    
    return 0;
}
\end{lstlisting}

This example demonstrates the basic operations and operations of the Eigen matrix. To compile it, you need to specify the header file directory of Eigen in CMakeLists.txt:
\begin{lstlisting}[caption=slambook2/ch3/useEigen/CMakeLists.txt]
# Add header file
include_directories( "/usr/include/eigen3" )
\end{lstlisting}

Repeat, because the Eigen library only has header files, so you don't need to link the program to the library with the target\_link\_libraries statement. However, for most other libraries, most of the time you need to use the link command. The approach here is not necessarily the best, because others may have Eigen installed in different locations, then you must manually modify the header file directory here. In the rest of the work, we will use the find\_ package command to search the library, but for the time being in this lecture. After compiling this program, run it and you can see the output of each matrix.

\begin{lstlisting}[caption=Terminal input:]
% build/eigenMatrix
matrix 2x3 from 1 to 6: 
1 2 3
4 5 6
print matrix 2x3: 
1	2	3	
4	5	6	
[1,2,3;4,5,6]*[3,2,1]=10 28
[1,2,3;4,5,6]*[4,5,6]: 32 77
random matrix: 
0.680375   0.59688 -0.329554
-0.211234  0.823295  0.536459
0.566198 -0.604897 -0.444451
transpose: 
0.680375 -0.211234  0.566198
0.59688  0.823295 -0.604897
-0.329554  0.536459 -0.444451
sum: 1.61307
trace: 1.05922
times 10: 
6.80375   5.9688 -3.29554
-2.11234  8.23295  5.36459
5.66198 -6.04897 -4.44451
inverse: 
-0.198521   2.22739    2.8357
1.00605 -0.555135  -1.41603
-1.62213   3.59308   3.28973
it: 0.208598
\end{lstlisting}

Since the detailed comments are given in the code, each line of the statement is not explained here. In this book, we will only give a description of several important places (the latter part will also maintain this style).

\begin{enumerate}
    \item Please enter the above code by yourself if you are a beginner in C++ (not including comments). At least compile and run the above program for once.
    
    \item Kdevelop may not prompt C++ member operations, which is caused by its incompleteness. Please follow the above to enter, do not care if it prompts an error. Clion will give you a complete hint.
    
    \item The matrix provided by  Eigen is very similar to MATLAB, and almost all data is treated as a matrix. However, in order to achieve better efficiency, you need to specify the size and type of the matrix in Eigen. For matrices that know the size at compile time, they are processed faster than dynamically changing matrices. Therefore, data such as rotation matrices and transformation matrices can be determined at compile times by their size and data type.
    
    \item The matrix implementation inside  Eigen is more complicated. I won't introduce it here. We hope that you can use Eigen's matrix like the built-in data types like float and double. This should be in line with the original intention of its design.
    
    \item The  Eigen matrix does not support automatic type promotion, which is quite different from C++'s built-in data types. In a C++ program, we can add and multiply a float variable and double variable, and \textbf{the compiler will automatically cast the data type to the most appropriate one}. In Eigen, for performance reasons, you must \textbf{explicitly} convert the matrix type. And if you forget to do this, Eigen will (not very friendly) prompt you with a very long "YOU MIXED DIFFERENT NUMERIC TYPES ..." compilation error. You can try to find out which part of the error message this message appears in. If the error message is too long, it is best to save it to a file and find it.
    
    \item Is the same, in the calculation process also need to ensure the correctness of the matrix dimension, otherwise there will be "YOU MIXED MATRICES OF DIFFERENT SIZES" error. Please don't complain about this kind of error prompting. For C++ template meta-programming, it is very lucky to be able to prompt the information that can be read. Later, if you find some compilation error about Eigen, you can directly look for the uppercase part and figure out what the problem is.
    
    \item Our routines only cover basic matrix operations. You can read more about Eigen by reading the Eigen official website tutorial: \\ { \url{http://eigen.tuxfamily.org/dox-devel/modules.html} }. Only the simplest part is demonstrated here. It is not equal to the fact that you can understand Eigen.
\end{enumerate}

In the last piece of code, the efficiency of inversion and QR decomposition is compared. You can look at the time difference on your own machine. Is there a significant difference between the two methods?
