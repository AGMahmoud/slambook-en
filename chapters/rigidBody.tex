% !Mode:: "TeX:UTF-8"
\chapter{3D Rigid Body Motion}

\begin{mdframed}
	\textbf{Goal of Study}
	\begin{enumerate}
		\item Understand the description of rigid body motion in three-dimensional space: rotation matrix, transformation matrix, quaternion and Euler angle.
		\item Understand the matrix and geometry module usage of the Eigen library.
	\end{enumerate}
\end{mdframed}

In the last lecture, we explained the framework and content of visual SLAM. This lecture will introduce one of the basic problems of visual SLAM: \textbf{ How to describe the motion of a rigid body in three-dimensional space?} Intuitively, we certainly know that this consists of one rotation plus one translation. Translation does not really have much problem, but the processing of rotation is a hassle. We will introduce the meaning of rotation matrices, quaternions, Euler angles, and how they are computed and transformed. In the practice section, we will introduce the linear algebra library Eigen. It provides a C++ matrix calculation, and its Geometry module also provides the structure described quaternion like rigid body motion. Eigen's optimization is perfect, but there are some special places to use it, we will leave it to the program.

\section{Rotation Matrix}
\label{sec:3.1}
\subsection{Point, Vector and Coordinate System}
The space in our daily life is three-dimensional, so we are born to be used to the movement of three-dimensional space. The three-dimensional space consists of three axes, so the position of one spatial point can be specified by three coordinates. However, we should now consider \textbf{rigid body} , which has not only its position, but also its own posture. The camera can also be viewed as a rigid body in three dimensions, so the position is where the camera is in space, and the attitude is the orientation of the camera. Combined, we can say, "The camera is in the space $ ( 0, 0 , 0 ) $ point, facing the front". But this natural language is cumbersome, and we prefer to describe it in a mathematical language.

We start with the most basic content: \textbf{points} and \textbf{vectors}. Points are the basic elements in space, no length, no volume. Connecting the two points forms a vector. A vector can be thought of as an arrow pointing from one point to another. We need to remind the reader that, please do not confuse the vector with its \textbf{coordinates}. A vector is one of the things in space, such as $ \mathbf{a}$ . Here $ \mathbf{a} $ does not need to be associated with several real numbers. Only when we specify a \textbf{coordinate system} in this three-dimensional space can we talk about the coordinates of the vector in this coordinate system, that is, find several real numbers corresponding to this vector.

With the knowledge of linear algebra, the coordinates of a point in 3D space can also be described by $ \mathbb{R}^3$. How to describe it? Suppose that in this linear space, we find a set of \textbf{base} \footnote{Just a reminder here, the base is a set of linearly independent vectors in the space, normally being orthognal and has unit-length.} $ (\mathbf{e}_1,\mathbf{e}_2,\mathbf{e}_3) $ , then, the arbitrary vector $ \mathbf{a} $ has a \textbf{coordinate} under this set of bases:

\begin{equation}
\mathbf{a} = \left[ {{\mathbf{e}_1},{\mathbf{e}_2},{\mathbf{e}_3}} \right]\left[ \begin{array}{l}
{a_1}\\
{a_2}\\
{a_3}
\end{array} \right] = {a_1}{\mathbf{e}_1} + {a_2}{\mathbf{e}_2} + {a_3}{\mathbf{e}_3}.
\end{equation}

Here $ (a_ 1 , a_ 2 , a_ 3 )^ \mathrm {T} $ is called $\mathbf {a}$'s coordinates \footnote {We use column vectors in this book which is same as most of the  mathematics books.}. The specific values of the coordinates are related to the vector itself, and also the selection of the bases. In $\mathbb{R}^3$, the coordinate system usually consists of 3 orthogonal coordinate axes (although it can also be non-orthogonal, it is rare in practice). For example, given $ \mathbf {x} $ and $ \mathbf {y} $ axis, the $ \mathbf {z} $ axis can be found using the right-hand (or left-hand) rule by $ \mathbf {x} \times  \mathbf {y} $. According to different definitions, the coordinate system is divided into left-handed and right-handed. The third axis of the left hand system is opposite to the right hand system. Most 3D libraries use right-handed (such as OpenGL, 3D Max, etc.), and some libraries use left-handed (such as Unity, Direct3D, etc.).

Based on basic linear algebra knowledge, we can talk about the operations between vectors/vectors, and vectors/numbers, such as scalar multiplication, vector addition, subtraction, inner product, outer product, and so on. Multiplication, addition and subtraction are fairly basic and intuitive. For example, the result of adding two vectors is to add their respective coordinates, subtraction, and so on. I won't go into details here. Internal and external products may be somewhat unfamiliar to the reader, and their calculations are given here. For $ \mathbf {a}, \mathbf {b} \in  \mathbb {R}^ 3 $ , in the usual sense \footnote {the inner product also has formal rules, but this book only discusses the usual inner product.} , the inner product of $\mathbf{a}, \mathbf{b}$ can be written as:

\begin{equation}
\mathbf{a} \cdot \mathbf{b} = { \mathbf{a}^\mathrm{T}}\mathbf{b} = \sum\limits_{i = 1}^3 {{a_i}{b_i}}  = \left| \mathbf{a} \right|\left| \mathbf{b} \right|\cos \left\langle {\mathbf{a},\mathbf{b}} \right\rangle ,
\end{equation}
where $ \left \langle { \mathbf {a}, \mathbf {b}} \right \rangle $ refers to the angle between the vector $ \mathbf {a}, \mathbf {b} $ . The inner product can also describe the projection relationship between vectors. The outer product is like this:

\begin{equation}
\label{eq:cross}
\mathbf{a} \times \mathbf{b} = \left\| {\begin{array}{*{20}{c}}
	\mathbf{e}_1 & \mathbf{e}_2 & \mathbf{e}_3 \\
	{{a_1}}&{{a_2}}&{{a_3}}\\
	{{b_1}}&{{b_2}}&{{b_3}}
	\end{array}} \right\| = \left[ \begin{array}{l}
{a_2}{b_3} - {a_3}{b_2}\\
{a_3}{b_1} - {a_1}{b_3}\\
{a_1}{b_2} - {a_2}{b_1}
\end{array} \right] = \left[ {\begin{array}{*{20}{c}}
	0&{ - {a_3}}&{{a_2}}\\
	{{a_3}}&0&{-{a_1}}\\  
	{-{a_2}}&{{a_1}}&0  
	\end{array}} \right] \mathbf{b} \buildrel \Delta \over = { \mathbf{a}^ \wedge } \mathbf{b}.
\end{equation}

The result of the outer product is a vector whose direction is perpendicular to the two vectors, and the length is $ \left | \mathbf{a} \right | \left | \mathbf{b} \right | \left \langle { \mathbf {a}, \mathbf {b}} \right \rangle  $, which is also the area of the quadrilateral of the two vectors. For the outer product operations, we introduce the $ ^ \wedge $ operator here, which means writing $ \mathbf{a} $ as a matrix. In fact, it is a \textbf {skew-symmetric matrix}\footnote{Skew-symmetric matrix means $ \mathbf{A} $ satisfies $ \mathbf{A}^ \mathrm{T}=- \mathbf{A}$. }. You can take $ ^ \wedge $ as an skew-symmetric symbol. It turns the outer product $ \mathbf{a} \times  \mathbf{b} $ into the multiplication of the matrix and the vector $ { \mathbf{a}^ \wedge } \mathbf{b} $ , which turns it into a linear operator. This symbol will be used frequently in the following sections, and this symbol is a one-to-one mapping, meaning that for any vector, it corresponds to a unique anti-symmetric matrix, and vice versa:

\begin{equation}
\mathbf{a}^\wedge = \left[ {\begin{array}{*{20}{c}}
	0&{-{a_3}}&{{a_2}}\\  
	{{a_3}}&0&{ - {a_1}}\\
	{ - {a_2}}&{{a_1}}&0
	\end{array}} \right].
\end{equation}

At the same time, note that the vector operations such as addition, subtraction, internal and external products, can be calculated even when we do not have their coordinates. For example, although the inner product can be expressed by the sum of the product products of the two vectors when we know the coordinates, it can also be calculated by the length and the angle even if their coordinates are unknown. Therefore, the inner product result of the two vectors is independent of the selection of the coordinate system.

\subsection{Euclidean Transforms}
We often define a variety of coordinate systems in the real scene. In robotics, you define one coordinate system for each link and joint; in 3D mapping, we also define the coordinate system for each cuboid and cylinder. If we consider a moving robot, it is common practice to set an inertial coordinate system (or world coordinate system) that can be considered stationary, such as the $x_W, y_W, z_W$ defined in Fig.~\ref{fig:axisTransform}. At the same time, the camera or robot is a moving coordinate system, such as the coordinate system defined by $x_C, y_C, z_C$. We might ask: a vector $\mathbf{p}$ in the camera's field of view, has coordinates $\mathbf{p}_c$ in the camera coordinate system; and in the world coordinate system, its coordinates are $ \mathbf{p}_w$, then how is the conversion between these two coordinates? At this time, it is necessary to first obtain the coordinate value of the point for the robot coordinate system, and then according to the robot pose \textbf{transform} into the world coordinate system. We need a mathematical way to describe this transformation. As we will see later, we can describe it with a matrix $\mathbf{T}$.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.7\textwidth]{rigidMotion/axisTransform.pdf}
    \caption {Coordinate transformation. For the same vector $ \mathbf{p}$ , it coordinates in the world $\mathbf{p}_W$ and coordinates in the camera system $ \mathbf{p}_C$ is different. This transformation relationship is described by the transform matrix $ \mathbf{T} $ . }
    \label{fig:axisTransform}
\end{figure}

Intuitively, the motion between two coordinate systems consists of a rotation plus a translation, which is called \textbf {rigid body motion}. Obviously, the camera movement is a rigid body one. During the rigid body motion, the length and angle of the a vector will not change. Imagine you throw your phone into the air and \footnote {Please don't put it into practice because it will fall on the ground and crash.}, there may only be differences in spatial position and posture, and its own length, angle of each face, etc. will not change. The phone will not be squashed like an eraser or be stretched during this motion. At this point, we say that the phone's motion is a \textbf {Euclidean Transform}.

The Euclidean transform consists of rotation and translation. Let's first consider about the rotation. We have an unit-length orthogonal base $ ( \mathbf {e}_ 1 , \mathbf {e}_ 2 , \mathbf {e}_ 3 ) $. After a rotation it becomes $ ( \mathbf {e}_ 1 ' , \mathbf {e}_ 2 ', \mathbf {e}_ 3 ') $ . Then, for the same vector $ \mathbf {a} $ (the vector does not move with the rotation of the coordinate system), its coordinates in two coordinate systems are $ [a_ 1 , a_ 2 , a_ 3 ] ^ \mathrm {T} $ and $[a'_ 1 , a'_ 2 , a'_ 3 ]^ \mathrm {T} $ . Because the vector itself has not changed, according to the definition of coordinates, there are:

\begin{equation}
\left[ \mathbf{e}_1,\mathbf{e}_2,\mathbf{e}_3 \right]\left[ \begin{array}{l}
{a_1}\\
{a_2}\\
{a_3}
\end{array} \right] = \left[ \mathbf{e}_1', \mathbf{e}_2', \mathbf{e}_3' \right]\left[ \begin{array}{l}
a'_1\\
a'_2\\
a'_3
\end{array} \right].
\end{equation}

To describe the relationship between the two coordinates, we multiply the left and right sides of the above equation by $ \left [ \begin {array}{l}
\mathbf{e}_1^\mathrm{T}\\
\mathbf{e}_2^\mathrm{T}\\
\mathbf{e}_3^\mathrm{T}
\end {array} \right ] $ , then the coefficient on the left becomes the identity matrix, so:

\begin{equation}
\left[ \begin{array}{l}
{a_1}\\
{a_2}\\
{a_3}
\end{array}\right]=\left[{\begin{array}{*{20}{c}}    
    {\mathbf{e}_1^\mathrm{T}\mathbf{e}_1'} & {\mathbf{e}_1^\mathrm{T}\mathbf{e}_2'} & {\mathbf{e}_1^\mathrm{T}\mathbf{e}_3'}\\
    {\mathbf{e}_2^\mathrm{T}\mathbf{e}_1'} & {\mathbf{e}_2^\mathrm{T}\mathbf{e}_2'} & {\mathbf{e}_2^\mathrm{T}\mathbf{e}_3'}\\
    {\mathbf{e}_3^\mathrm{T}\mathbf{e}_1'} & {\mathbf{e}_3^\mathrm{T}\mathbf{e}_2'} & {\mathbf{e}_3^\mathrm{T}\mathbf{e}_3'}
    \end{array}} \right]\left[ \begin{array}{l}
a_1'\\
a_2'\\
a_3'
\end{array} \right] \buildrel \Delta \over = \mathbf{R} \mathbf{a}'.
\end{equation}
We take the intermediate matrix out and define it as a matrix $ \mathbf{R} $ . This matrix consists of the inner product between the two sets of bases, describing the coordinate transformation relationship of the same vector before and after the rotation. As long as the rotation is the same, this matrix is the same. It can be said that the matrix $ \mathbf{R} $ describes the rotation itself. So we call it the \textbf{rotation matrix}. At the same time, the components of the matrix are the inner product of the two coordinate system bases. Since the length of the base vector is 1, it is actually the cosine of the angle between the base vectors. So this matrix is also called \textbf{Direction Cosine Matrix}. We will call it the rotation matrix in the following.

The rotation matrix has some special properties. In fact, it is an orthogonal matrix with a determinant of 1 \footnote{Orthogonal matrix is a matrix whose inverse is its transpose. The orthogonality of the rotation matrix can be derived directly from the definition. } \footnote{The determinant is 1 is artificially defined. In fact, its determinant is $\pm 1 $, but the rotation with determinant $ - 1 $ is called improper rotation, that is, one rotation plus one reflection. }. Conversely, an orthogonal matrix with a determinant of 1 is also a rotation matrix. So, you can define a collection of $n$ dimensional rotation matrices as follows:
\begin{equation}
\mathrm{SO}(n) = \{ \mathbf{R} \in \mathbb{R}^{n \times n} | \mathbf{R R}^\mathrm{T} = \mathbf{I}, \mathrm{det} (\mathbf{R})=1 \}.
\end{equation}

$\mathrm{SO}(n) $ is the meaning of \textbf {Special Orthogonal Group}. We leave the contents of the ``group'' to the next lecture. This collection consists of a rotation matrix of $ n $ dimensional space, in particular, $\mathrm {SO}(3)$ refers to the rotation of the three-dimensional space. By this way, we can talk directly about the rotation transformation between the two coordinate systems without having to start from the bases.

Since the rotation matrix is an orthogonal matrix, its inverse (ie, transpose) describes an opposite rotation. According to the above definition, there are:
\begin{equation}
\mathbf{a} '= \mathbf{R}^{-1} \mathbf{a} = \mathbf{R}^ \mathrm{T} \mathbf{a}.
\end{equation}
Obviously $ \mathbf{R}^\mathrm{T} $ portrays an opposite rotation.

In the Euclidean transformation, there is translation in addition to rotation. Consider the vector $ \mathbf{a} $ in the world coordinate system , after a rotation (depicted by $ \mathbf{R} $) and a translation of $ \mathbf{t} $ , you get $ \mathbf{a}' $ , then put the rotation and translation together, there are:
\begin{equation}
\label{eq:RT}
\mathbf{a} '= \mathbf{R} \mathbf{a} + \mathbf{t}.
\end{equation}
Where $ \mathbf{t} $ is called a translation vector. Compared to rotation, the translation part simply adds the translation vector to the coordinates after the rotation, which is very simple. By the above formula, we completely describe the coordinate transformation relationship of an Euclidean space using a rotation matrix $ \mathbf{R} $ and a translation vector $ \mathbf{t}$. In practice, we will define the coordinate system 1, coordinate system 2, then the vector $ \mathbf{a} $ under the two coordinates is $ \mathbf{a}_1 , \mathbf{a}_2 $ , they are The relationship between the two, in accordance with the complete writing, should be:
\begin{equation}
\mathbf{a}_1 = \mathbf{R}_{12} \mathbf{a}_2 + \mathbf{t}_{12}.
\end{equation}
Here $ \mathbf{R}_{12} $ means ``rotation the vector from coordinate system 2 to coordinate system 1''. Since the vector is multiplied to the right of this matrix, its subscript is \textbf{read from right to left}. This is just a customary way of writing this book. Coordinate transformations are easy to confuse, especially if multiple coordinate systems exist. Similarly, if we want to express ``rotation matrix from 1 to 2'', we write it as $ \mathbf{R}_{21}$. The reader must be clear about the notation here, because different books have different notations, some will be recorded as the top left/subscript, and the text will be written on the right side.

About $\mathbf{t}_{12}$ , it actually corresponds a vector from the coordinate system 1 origin pointing to the coordinate system 2 origin, whose \textbf{coordinates are taken under coordinate system 1}, so I suggest readers to put it as ``a vector from 1 to 2''. But the reverse $ \mathbf{t}_{21} $ , which is a vector from 2's origin to 1's origin, whose \textbf{coordinates are taken in coordinate system 2}, is not equal to $-\mathbf{t}_{12}$, but is also related to the rotation of the two systems\footnote{Although from the vector level, they are indeed inverse relations, but the coordinates of the two vectors are not opposite. Can you find out why it looks like this? }. Therefore, when beginners ask the question ``Where is my coordinates?'', we need to clearly explain the meaning of this sentence. Here ``my coordinates'' normally refers to the vector from the world system pointing to the origin of the robot system, and then take the coordinates in the world's base. Corresponding to the mathematical symbol, it should be the value of $ \mathbf{t}_{WC} $ . For the same reason, it is not $ - \mathbf {t}_{CW}$, but actually $-\mathbf{R}_{CW}^T \mathbf{t}_{CW}$.

\subsection{Transform Matrix and Homogeneous Coordinates}
The formula \eqref{eq:RT} fully expresses the rotation and translation of Euclidean space, but there is still a small problem: the transformation relationship here is not a linear relationship. Suppose we made two transformations: $ \mathbf {R}_ 1 , \mathbf {t}_ 1 $ and $ \mathbf {R}_ 2 , \mathbf {t}_ 2 $:

\[
\mathbf{b} = {\mathbf{R}_1} \mathbf{a} + {\mathbf{t}_1}, \quad \mathbf{c} = {\mathbf{R}_2} \mathbf{b} + {\mathbf{t}_2}.
\]
So, the transformation from $ \mathbf{a} $ to $ \mathbf{c} $ is:
\[
\mathbf{c} = {\mathbf{R}_2}\left( {{\mathbf{R}_1} \mathbf{a} + {\mathbf{t}_1}} \right) + {\mathbf{t}_2}.
\]
This form is not elegant after multiple transformations. Therefore, we introduce homogeneous coordinates and transformation matrices, rewriting the form \eqref{eq:RT}:
\begin{equation}
\left[\begin{array}{l} 
\mathbf {a} ' \\
1
\end{array} \right] = 
\left[ {\begin{array}{*{20}{c}}
    \mathbf{R}&\mathbf{t}\\
    {{\mathbf{0}^\mathrm{T}}}&1
    \end{array}} \right]
\left[ \begin{array}{l}
\mathbf {a} \\
1
\end{array} \right]  \buildrel \Delta \over = \mathbf{T} \left[ \begin{array}{l}
\mathbf {a} \\
1
\end{array} \right].
\end{equation}

This is a mathematical trick: we add $ 1 $ at the end of a 3D vector and turn it into a 4D vector called \textbf{homogeneous coordinates}. For this four-dimensional vector, we can write the rotation and translation in one matrix, making the whole relationship a linear relationship. In this formula, the matrix $ \mathbf {T} $ is called \textbf{Transform Matrix}.

We temporarily use $  \tilde { \mathbf {a} } $ to represent the homogeneous coordinates of $ \mathbf {a} $. Then, relying on homogeneous coordinates and transformation matrices, the superposition of the two transformations can have a good form:
\begin{equation}
\tilde{\mathbf{b}} = \mathbf{T}_1 \tilde{\mathbf{a}}, \  \tilde{\mathbf{c}} = \mathbf{T}_2 \tilde{\mathbf{b}} \quad \Rightarrow \tilde{\mathbf{c}} = \mathbf{T}_2 \mathbf{T_1} \tilde{\mathbf{a}}.
\end{equation}
But the symbols that distinguish between homogeneous and non-homogeneous coordinates make us annoyed, because here we only need to add 1 at the end of the vector or remove 1 to make it a normal one\footnote {But the purpose of the homogeneous coordinates is not limited to this, we will come back to it in Chapter 7.}. So, without ambiguity, we will write it directly as $ \mathbf {b}= \mathbf {T} \mathbf {a} $ , and by default we just assume a homogeneous coordinate conversion is made if needed\footnote { Note that if homogeneous coordinate transformation is not performed, the matrix multiplication here does not make sense. }.

Regarding the transformation matrix $ \mathbf{T} $, it has a special structure: the upper left corner is the rotation matrix, the right side is the translation vector, the lower left corner is $ \mathbf{0} $ vector, and the lower right corner is 1. This matrix is also known as the Special Euclidean Group:

\begin{equation}
\mathrm{SE}(3) = \left\{ \mathbf{T} = \left[ {\begin{array}{*{20}{c}}
    \mathbf{R} & \mathbf{t} \\
    {{\mathbf{0}^\mathrm{T}}} & 1
    \end{array}} \right]
\in \mathbb{R}^{4 \times 4} | \mathbf{R} \in \mathrm{SO}(3), \mathbf{t} \in \mathbb{R}^3\right\} .
\end{equation}

Like $ \mathrm{SO}( 3 ) $ , solving the inverse of the matrix represents an inverse transformation:

\begin{equation}
{ \mathbf{T}^{ - 1}} = \left[ {\begin{array}{*{20}{c}}
    {{\mathbf{R}^\mathrm{T}}}&{ - {\mathbf{R}^\mathrm{T}}\mathbf{t}}\\
    {{\mathbf{0}^\mathrm{T}}}&1
    \end{array}} \right].
\end{equation}

Again, we use the notation of $ \mathbf{T}_{12} $ to represent a transformation from 2 to 1. Moreover, in order to keep the symbol concise, in the case of no ambiguity, the symbols of the homogeneous coordinates and the ordinary coordinates are not deliberately distinguished in the later sections. For example, when we write $ \mathbf {T} \mathbf{a} $ , we use homogeneous coordinates (otherwise we can't calculate). When you write $ \mathbf{Ra} $ , you use non-homogeneous coordinates. If they are written in the same equation, it is assumed that the conversion from homogeneous coordinates to normal coordinates is already done - because the conversion between homogeneous and non-homogeneous coordinates is actually very easy. In C++ programs. You can do this with \textbf{operator overloading} to ensure that the operations you see in the program are correct.

Let's take a review now. First, we introduce the vector and its coordinate representation, and introduce the operation between the vectors; then, the motion between the coordinate systems is described by the Euclidean transformation, which consists of translation and rotation. The rotation can be described by the rotation matrix $ \mathrm{SO}( 3 ) $ , while the translation is directly described by a $ \mathbb{R}^ 3 $ vector. Finally, if the translation and rotation are placed in a matrix, the transformation matrix $ \mathrm{SE}( 3 ) $ is formed .

\section{Practice: Using Eigen}
The practical part of this lecture has two sections. In the first part, we will explain how to use Eigen to represent matrices and vectors, and then extend to the calculation of rotation matrix and transformation matrix. The code for this section is in \textbf{slambook2/ch3/useEigen}.

Eigen\footnote{Official home page: \url{http://eigen.tuxfamily.org/index.php?title=Main_Page}. } is a C++ open source linear algebra library. It provides fast linear algebra operations on matrices, as well as functions such as solving equations. Many upper-level software libraries also use Eigen for matrix operations, including g2o, Sophus, and others. In the theoretical part of this lecture, let's learn about Eigen's programming.

Eigen may not be installed on your PC. Please enter the following command to install it:

\begin{lstlisting}[language=sh,caption=Terminal input:]
sudo apt-get install libeigen3-dev
\end{lstlisting}

Most of the commonly used libraries in our book are available in the Ubuntu software source. Later, if you want to install a library, you may want to search for the Ubuntu software source. With the apt command, we can easily install Eigen. Looking back at the previous lesson, we know that a library consists of header files and library files. The default location of the Eigen header file should be in "/usr/include/eigen3/". If you are not sure, you can find it by entering the following command:

\begin{lstlisting}[language=sh,caption=Terminal input:]
sudo locate eigen3
\end{lstlisting}

Compared to other libraries, Eigen is special in that it is a library built with pure header files (this is amazing!). This means you can only find its header files, not binary files like .so or .a. When you use it, you only need to import Eigen's header file, you don't need to link the library file (because it doesn't have a library file). Write a piece of code below to actually practice the use of Eigen:
\begin{lstlisting}[language=c++,caption=slambook2/ch3/useEigen/eigenMatrix.cpp]
#include <iostream>
using namespace std;

#include <ctime>
// Eigen core
#include <Eigen/Core>
// Algebraic operations of dense matrices (inverse, eigenvalues, etc.)
#include <Eigen/Dense>
using namespace Eigen;

#define MATRIX_SIZE 50

/****************************
* This program demonstrates the use of the basic Eigen type
****************************/

int main(int argc, char **argv) {
    // All vectors and matrices in Eigen are Eigen::Matrix, which is a template
    // class. Its first three parameters are: data type, row, column Declare a 2*3
    // float matrix
    Matrix<float, 2, 3> matrix_23;
    
    // At the same time, Eigen provides many built-in types via typedef, but the
    // bottom layer is still Eigen::Matrix For example, Vector3d is essentially
    // Eigen::Matrix<double, 3, 1>, which is a three-dimensional vector.
    Vector3d v_3d;
    // This is the same
    Matrix<float, 3, 1> vd_3d;
    
    // Matrix3d is essentially Eigen::Matrix<double, 3, 3>
    Matrix3d matrix_33 = Matrix3d::Zero(); // initialized to zero
    // If you are not sure about the size of the matrix, you can use a matrix of
    // dynamic size
    Matrix<double, Dynamic, Dynamic> matrix_dynamic;
    // simpler
    MatrixXd matrix_x;
    // There are still many types of this, we doesn't list them one by one.
    
    // Here is the operation of the Eigen array
    // input data (initialization)
    matrix_23 << 1, 2, 3, 4, 5, 6;
    // output
    cout << "matrix 2x3 from 1 to 6: \n" << matrix_23 << endl;
    
    // Use () to access elements in the matrix
    cout << "print matrix 2x3: " << endl;
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 3; j++)
        cout << matrix_23(i, j) << "\t";
        cout << endl;
    }
    
    // The matrix and vector are multiplied (actually still matrices and matrices)
    v_3d << 3, 2, 1;
    vd_3d << 4, 5, 6;
    
    // But in Eigen you can't mix two different types of matrices, like this is
    // wrong Matrix<double, 2, 1> result_wrong_type = matrix_23 * v_3d; should be
    // explicitly converted
    Matrix<double, 2, 1> result = matrix_23.cast<double>() * v_3d;
    cout << "[1,2,3;4,5,6]*[3,2,1]=" << result.transpose() << endl;
    
    Matrix<float, 2, 1> result2 = matrix_23 * vd_3d;
    cout << "[1,2,3;4,5,6]*[4,5,6]: " << result2.transpose() << endl;
    
    // Also you can't misjudge the dimensions of the matrix
    // Try canceling the comments below to see what Eigen will report.
    // Eigen::Matrix<double, 2, 3> result_wrong_dimension =
    // matrix_23.cast<double>() * v_3d;
    
    // some matrix operations
    // Four operations are not demonstrated, just use +-*/.
    Matrix_33 = Matrix3d::Random(); // Random Number Matrix
    cout << "random matrix: \n" << matrix_33 << endl;
    cout << "transpose: \n" << matrix_33.transpose() << endl;
    cout << "sum: " << matrix_33.sum() << endl;
    cout << "trace: " << matrix_33.trace() << endl;
    cout << "times 10: \n" << 10 * matrix_33 << endl;
    cout << "inverse: \n" << matrix_33.inverse() << endl;
    cout << "det: " << matrix_33.determinant() << endl;
    
    // Eigenvalues
    // Real symmetric matrix can guarantee successful diagonalization
    SelfAdjointEigenSolver<Matrix3d> eigen_solver(matrix_33.transpose() *
    matrix_33);
    cout << "Eigen values = \n" << eigen_solver.eigenvalues() << endl;
    cout << "Eigen vectors = \n" << eigen_solver.eigenvectors() << endl;
    
    // Solving equations
    // We solve the equation of matrix_NN * x = v_Nd
    // The size of N is defined in the previous macro, which is generated by a
    // random number Direct inversion is the most direct, but the amount of
    // inverse operations is large.
    
    Matrix<double, MATRIX_SIZE, MATRIX_SIZE> matrix_NN =
    MatrixXd::Random(MATRIX_SIZE, MATRIX_SIZE);
    matrix_NN =
    matrix_NN * matrix_NN.transpose(); // Guarantee semi-positive definite
    Matrix<double, MATRIX_SIZE, 1> v_Nd = MatrixXd::Random(MATRIX_SIZE, 1);
    
    Clock_t time_stt = clock(); // timing
    // Direct inversion
    Matrix<double, MATRIX_SIZE, 1> x = matrix_NN.inverse() * v_Nd;
    cout << "time of normal inverse is "
    << 1000 * (clock() - time_stt) / (double)CLOCKS_PER_SEC << "ms" << endl;
    cout << "x = " << x.transpose() << endl;
    
    // Usually solved by matrix decomposition, such as QR decomposition, the speed
    // will be much faster
    time_stt = clock();
    x = matrix_NN.colPivHouseholderQr().solve(v_Nd);
    cout << "time of Qr decomposition is "
    << 1000 * (clock() - time_stt) / (double)CLOCKS_PER_SEC << "ms" << endl;
    cout << "x = " << x.transpose() << endl;
    
    // For positive definite matrices, you can also use cholesky decomposition to
    // solve equations.
    time_stt = clock();
    x = matrix_NN.ldlt().solve(v_Nd);
    cout << "time of ldlt decomposition is "
    << 1000 * (clock() - time_stt) / (double)CLOCKS_PER_SEC << "ms" << endl;
    cout << "x = " << x.transpose() << endl;
    
    return 0;
}
\end{lstlisting}

This example demonstrates the basic operations and operations of the Eigen matrix. To compile it, you need to specify the header file directory of Eigen in CMakeLists.txt:
\begin{lstlisting}[caption=slambook2/ch3/useEigen/CMakeLists.txt]
# Add header file
include_directories( "/usr/include/eigen3" )
\end{lstlisting}

Repeat, because the Eigen library only has header files, so you don't need to link the program to the library with the target\_link\_libraries statement. However, for most other libraries, most of the time you need to use the link command. The approach here is not necessarily the best, because others may have Eigen installed in different locations, then you must manually modify the header file directory here. In the rest of the work, we will use the find\_ package command to search the library, but for the time being in this lecture. After compiling this program, run it and you can see the output of each matrix.

\begin{lstlisting}[caption=Terminal input:]
% build/eigenMatrix
matrix 2x3 from 1 to 6: 
1 2 3
4 5 6
print matrix 2x3: 
1	2	3	
4	5	6	
[1,2,3;4,5,6]*[3,2,1]=10 28
[1,2,3;4,5,6]*[4,5,6]: 32 77
random matrix: 
0.680375   0.59688 -0.329554
-0.211234  0.823295  0.536459
0.566198 -0.604897 -0.444451
transpose: 
0.680375 -0.211234  0.566198
0.59688  0.823295 -0.604897
-0.329554  0.536459 -0.444451
sum: 1.61307
trace: 1.05922
times 10: 
6.80375   5.9688 -3.29554
-2.11234  8.23295  5.36459
5.66198 -6.04897 -4.44451
inverse: 
-0.198521   2.22739    2.8357
1.00605 -0.555135  -1.41603
-1.62213   3.59308   3.28973
it: 0.208598
\end{lstlisting}

Since the detailed comments are given in the code, each line of the statement is not explained here. In this book, we will only give a description of several important places (the latter part will also maintain this style).

\begin{enumerate}
    \item Please enter the above code by yourself if you are a beginner in C++ (not including comments). At least compile and run the above program for once.
    
    \item Kdevelop may not prompt C++ member operations, which is caused by its incompleteness. Please follow the above to enter, do not care if it prompts an error. Clion will give you a complete hint.
    
    \item The matrix provided by  Eigen is very similar to MATLAB, and almost all data is treated as a matrix. However, in order to achieve better efficiency, you need to specify the size and type of the matrix in Eigen. For matrices that know the size at compile time, they are processed faster than dynamically changing matrices. Therefore, data such as rotation matrices and transformation matrices can be determined at compile times by their size and data type.
    
    \item The matrix implementation inside  Eigen is more complicated. I won't introduce it here. We hope that you can use Eigen's matrix like the built-in data types like float and double. This should be in line with the original intention of its design.
    
    \item The  Eigen matrix does not support automatic type promotion, which is quite different from C++'s built-in data types. In a C++ program, we can add and multiply a float variable and double variable, and \textbf{the compiler will automatically cast the data type to the most appropriate one}. In Eigen, for performance reasons, you must \textbf{explicitly} convert the matrix type. And if you forget to do this, Eigen will (not very friendly) prompt you with a very long "YOU MIXED DIFFERENT NUMERIC TYPES ..." compilation error. You can try to find out which part of the error message this message appears in. If the error message is too long, it is best to save it to a file and find it.
    
    \item Is the same, in the calculation process also need to ensure the correctness of the matrix dimension, otherwise there will be "YOU MIXED MATRICES OF DIFFERENT SIZES" error. Please don't complain about this kind of error prompting. For C++ template meta-programming, it is very lucky to be able to prompt the information that can be read. Later, if you find some compilation error about Eigen, you can directly look for the uppercase part and figure out what the problem is.
    
    \item Our routines only cover basic matrix operations. You can read more about Eigen by reading the Eigen official website tutorial: \\ { \url{http://eigen.tuxfamily.org/dox-devel/modules.html} }. Only the simplest part is demonstrated here. It is not equal to the fact that you can understand Eigen.
\end{enumerate}

In the last piece of code, the efficiency of inversion and QR decomposition is compared. You can look at the time difference on your own machine. Is there a significant difference between the two methods?

\section{Rotation Vector and Euler Angle}
\subsection{Rotation Vector}
Now let's return to the theoretical part. With a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to describe a 6-degree-of-freedom 3D rigid body motion? Obviously the matrix representation has at least the following disadvantages:
\begin{enumerate}
    \item  $\mathrm{SO}( 3 ) $ has a rotation matrix of 9 quantities, but a 3D rotation only has 3 degrees of freedom. Therefore the matrix expression is redundant. Similarly, the transformation matrix expresses a 6 degree-of-freedom transformation with 16 quantities. So, is there a more compact representation?
    \item The rotation matrix itself has constraints: it must be an orthogonal matrix with a determinant of 1. The same is true for the transformation matrix. These constraints make the solution more difficult when you want to estimate or optimize a rotation matrix/transform matrix.
\end{enumerate}

Therefore, we hope that there is a way to describe rotation and translation in a compact manner. For example, is it feasible to express rotation with a three-dimensional vector and express transformation with a six-dimensional vector? In fact, a rotation can be described by \textbf {a rotation axis and a rotation angle}. Thus, we can use a vector whose direction is parallel with the axis of rotation and the length is equal to the angle of rotation, which is called the \textbf {rotation vector} (or Angle-Axis/Axis-Angle), and only a three-dimensional vector is needed to describe the rotation. Similarly, for a transformation matrix, we use a rotation vector and a translation vector to express a transformation. The variable dimension at this time is exactly six dimensions.

Consider a rotation represented by $ \mathbf{R} $. If described by a rotation vector, assuming that the rotation axis is a unit length vector $ \mathbf{n} $ and the angle is $ \theta $, then the vector $ \theta \mathbf{n} $ can also describe this rotation. So, we have to ask, what is the connection between the two expressions? In fact, it is not difficult to derive their conversion relationship. The conversion process from the rotation vector to the rotation matrix is shown by \textbf{Rodrigues's Formula}. Since the derivation process is a little complicated, it is not described here. Only the result of the conversion is given\footnote{For interested readers, please refer to \url{https://en.wikipedia.org/wiki/Rodrigues \% 27_rotation_formula}, in fact the next chapter will give a proof from the Lie algebra view.}:

\begin{equation}
\label{eq:rogridues}
\mathbf{R} = \cos \theta \mathbf{I} + \left({ 1 - \cos \theta } \right) \mathbf{n} { \mathbf {n} ^ \mathrm{T} } + \sin \theta { \mathbf{n}^ \wedge }.
\end{equation}

The symbol $ ^ \wedge $ is a vector to skew-symmetric conversion, see the formula \eqref{eq:cross}. Conversely, we can also calculate the conversion from a rotation matrix to a rotation vector. For the corner $ \theta $, take the \textbf{trace} \footnote {see \textbf{trace} on both sides to find the sum of the diagonal elements of the matrix. }, we have:

\begin{equation}
\begin{aligned}
\mathrm{tr} \left( \mathbf{R} \right) &= \cos \theta \mathop{}\!\mathrm{tr}\left( \mathbf{I} \right) + \left( {1 - \cos \theta } \right) \mathop{}\!\mathrm{tr} \left( { \mathbf{n} {\mathbf{n}^\mathrm{T}}} \right) + \sin \theta \mathop{}\!\mathrm{tr} ({\mathbf{n}^ \wedge })\\
&= 3\cos \theta  + (1 - \cos \theta )\\
&= 1 + 2\cos \theta .
\end{aligned} 
\end{equation}

therefore:
\begin{equation}
\label{eq:R2theta}
\theta = \arccos ( \frac{\mathrm{tr}(\mathbf{R}) - 1}{2}  ) .
\end{equation}

Regarding the axis $ \mathbf{n} $ , since the vector on the rotation axis does not change after the rotation, it means:
\begin{equation}
\mathbf{R} \mathbf{n} = \mathbf{n}.
\end{equation}

Therefore, the axis $ \mathbf{n} $ is the eigen vector corresponding to the matrix $ \mathbf{R}$'s eigenvalue 1. Solving this equation and normalizing it gives the axis of rotation. By the way, the two conversion formulas here will still appear in the next lecture, and you will find that they are exactly the correspondence between Lie group and Lie algebra on $ \mathrm{SO}(3) $ .

\subsection{Euler Angle}

Let's talk about the Euler angle.

Whether it is a rotation matrix or a rotation vector, although they can describe the rotation, they are very unintuitive to us humans. When we see a rotation matrix or a rotation vector, it is hard to imagine what this rotation is like. When they change, we don't know which direction the object is turning. The Euler angle provides a very intuitive way to describe rotation—it uses \textbf{3 primal axes} to decompose a rotation into three rotations around different axes. Humans can easily understand the process of rotating around a single axis. However, due to the variety of decomposition methods, there are many different and confusing definition methods for Euler angles. For example, we can first rotate around the $X$ axis, then around the $Y$ axis, and finally around the $Z$ axis, and by this way we get a rotation like $XYZ$ order. Similarly, you can define rotation orders such as $ZYZ$ and $ZYX$. You also need to distinguish whether it is rotated around the \textbf{fixed axis} or around the \textbf{axis after rotation}, which will also give a different definition.

This uncertainty in the axis orders brings many practical difficulties. Fortunately, in certain research areas, Euler angles usually have a uniform definition. You may have heard the words ``pitch angle'' and ``yaw angle'' of an aircraft. One of the most commonly used Euler angles is the yaw-pitch-roll angles. Since it is equivalent to the rotation of the $ZYX$ axis, the $ZYX$ is taken as an example. Suppose the front of a rigid body (toward our direction) is the $X$ axis, the right side is the $Y$ axis, and the top is the $Z$ axis, as shown by \autoref{fig:eulerAngles}. Then, the $ZYX$ angle is equivalent to decompose any rotation into the following three axes:

\begin{enumerate}
    \item Rotate around the $Z$ axis of the object to get the yaw angle $\theta_{\mathrm{yaw}}=y$;
    \item Rotate around the $Y$ axis of \textbf{after rotation} to get the pitch angle $\theta_{\mathrm{pitch}}=p$;
    \item Rotate around the $X$ axis of \textbf{after rotation} to get the roll angle $\theta_{\mathrm{roll}}=r$.
\end{enumerate}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{rigidMotion/eulerAngles.pdf}
    \caption{Euler angles. The top is defined for the ZYX order (rpy order). The bottoms shows when pitch=$90^\circ$, the third rotation is using the same axis as the first one, causing the system to lose a degree of freedom. If you don't understand the universal lock, please take a look at the related videos and it will be more convenient to understand. }
    \label{fig:eulerAngles}
\end{figure}

By this way, you can use a three-dimensional vector such as $[r,p,y]^\mathrm{T}$ to describe any rotation. This vector is very intuitive, we can imagine the rotation process from this vector. The other Euler angles are also decomposed into three axes to obtain a three-dimensional vector, but the axes and order maybe different. The $rpy$ angle introduced here is a widely used one, and only a few Euler angles have such a popular name as $rpy$. Different Euler angles are referred to in the order of the axes of rotation. For example, the rotation order of the rpy angle is $ZYX$. Similarly, there are Euler angles like $XYZ, ZYZ$ - but they don't have a specific name. It is worth mentioning that most areas have their own coordinate directions and order habits when using Euler angles, not necessarily the same as we said here.

A major drawback of Euler Angle is that it encounters the famous \textbf{Gimbal Lock} \footnote{\url{https://en.wikipedia.org/wiki/Gimbal_lock}.}): in $rpy$'s case, when the pitch angle is $\pm 90 ^\circ $, the first rotation and the third rotation will use the same axis, causing the system to lose a degree of freedom (from 3 rotations to 2 rotations). This is called the singularity problem and also exists in other forms of Euler Angles. In theory, it can be proved that as long as you want to use three real numbers to express the three-dimensional rotation, you will inevitably encounter the singularity problem\footnote{The rotation vector also has singularity, which occurs when the angle $\theta$ exceeds $2\pi$. Obviously rotating $2\pi$ is same with no rotation.}. Due to this principle, Euler angles are not suitable for interpolation and iteration, and are often only used in human-computer interaction. We also rarely use Euler angles to express poses directly in the SLAM program, nor do we use Euler angles to express rotation in filtering or optimization (because it has singularity). However, if you want to verify that your algorithm is correct or not, converting to Euler angles can help you quickly determine if the results are correct. In some cases where the main body is mainly 2D motion (such as sweepers, self-driving vehicles), we can also decompose the rotation into three Euler angles, and then take one of them (such as the yaw angle) as the positioning information output.

\section{Quaternion}

The rotation matrix describes the rotation of 3 degrees of freedom with 9 quantities, with redundancy; the Euler angles and the rotation vectors are compact but has singularity. In fact, we \textbf{cannot find a three-dimensional vector description without singularity}\cite{Stuelpnagel1964}. This is somewhat similar to using two coordinates to represent the Earth's surface (such as longitude and latitude), and there will be singularity (longitude is meaningless when latitude is $ \pm  90 ^ \circ $ ).

Recall the complex number that we have studied before. We use the complex set $ \mathbb {C} $ to represent the vector on the 2D complex plane, and the complex multiplication with an unit complex number can represent the rotation on the 2D plane: for example, multiplying the complex $i$ is equivalent to rotating a complex vector counterclockwise by $ 90 ^ \circ $. Similarly, when expressing a three-dimensional space rotation, there is also an algebra similar to a complex number: \textbf{quaternions}. The quaternion is an extended complex number found by Hamilton. It \textbf{ is both compact and not singular}. If we must find some shortcomings, the quaternion is not intuitive enough, and its operation is a bit more complicated.

Comparing quaternions to complex numbers can help you understand quaternions faster. For example, when we want to rotate the vector of a complex plane by $\theta$ , we can multiply this complex vector by $\mathrm{e}^{i \theta}$, which is a complex number represented by polar coordinates. It can also be written in the usual form like the famous Euler equation:
\begin{equation}
\mathrm{e}^{i\theta} = \cos \theta + i \sin \theta.
\end{equation}
This is a unit length complex number. Therefore, in the case of two dimensions, the rotation can be described by \textbf{unit complex number}. Similarly, we will see that 3D rotation can be described by a \textbf{unit quaternion}.

A quaternion $ \mathbf{q} $ has a real part and three imaginary parts. We write the real part in the front (and there are also some books where the real part is written in the last), like this:
\begin{equation}
\mathbf{q} = q_0 + q_1 i + q_2 j + q_3 k,
\end{equation}
Where $ i,j,k $ are the three imaginary parts of the quaternion. These three imaginary parts satisfy the following relationship:
\begin{equation}
\label{eq:quaternionVirtual}
\left\{ \begin{array}{l}
{i^2} = {j^2} = {k^2} =  - 1\\
ij = k, ji = - k \\
jk = i,kj =  - i\\
ki = j, i = - j
\end{array} \right. .
\end{equation}
If we look at $ i, j, k $ as three axes, they are the same as their own multiplications and complex numbers, and the multiplication and outer product are the same. Sometimes people also use a scalar and a vector to express quaternions:
\[
\mathbf{q} = \left[ s, \mathbf{v} \right]^\mathrm{T}, \quad s=q_0 \in \mathbb{R},\quad \mathbf{v} = [q_1, q_2, q_3]^\mathrm{T} \in \mathbb{R}^3,
\]
Here, $ s $ is the real part of the quaternion, and $ \mathbf {v} $ is its imaginary part. If the imaginary part of a quaternion is $ \mathbf {0} $ , it is called \textbf{real quaternion}. Conversely, if its real part is $ 0 $ , it is called \textbf{imaginary quaternion}.

We can use a \textbf{unit quaternion} to represent any rotation in 3D space, but this expression is subtly different from the complex numbers. In the complex, multiplying by $ i $ means rotating $ 90 ^ \circ $ . Does this mean that in the quaternion, multiplied by $ i $ is rotated around the $ i $ axis by $ 90 ^ \circ $ ? So, does $ ij = k $ mean, first rotating around the $ i $ by $ 90 ^ \circ $ , then around $j$ by $ 90 ^ \circ $ , is equivalent to rotating around $ k$ by $ 90 ^ \circ $ ? Readers can use a cell phone to simulate that, then you will find that this is not the case. The correct situation should be that multiplying $ i $ corresponds to rotating $ 180 ^ \circ $ , in order to guarantee the nature of $ ij=k $ . And $ i^ 2 =- 1 $ means that after rotating $ 360 ^ \circ $ around the $ i $ axis, we get an opposite thing. This thing has to be rotated for 720 $^\circ$ to be equal to its original appearance.

This seems a bit mysterious, the complete explanation needs too much extra things, let's calm down and come back to the quaternions. At least, we know that a unit quaternion can express the rotation of a three-dimensional space. So what are the properties of the quaternions? And how can they operate with each other?

\subsection{Quaternion Operations}

Quaternions are very similar to complex numbers, and a series of operations can be performed. We can easily plus, minus, multiplies to quaternions just like doing with two complex numbers. Assume there are two quaternions $ \mathbf{q}_a, \mathbf{q}_b $ , whose vectors are represented as $ [s_a, \mathbf {v}_a]^ \mathrm {T}, [s_b, \mathbf{v}_b]^ \mathrm {T} $ , or the original quaternion is expressed as:
\[
\mathbf{q} _a = s_a + x_ai + y_a + z_ak, \quad  \mathbf {q} _b = s_b + x_bi + y_bj + z_bk.
\]
Then, their operations can be expressed as follows.

\begin{enumerate}
    \item { \emph {Addition and Subtraction}.} The addition and subtraction of the quaternion $ \mathbf {q}_a, \mathbf {q}_b $ is:
    \begin{equation} 	
    \mathbf{q}_a \pm \mathbf{q}_b = \left[ s_a \pm s_b, \mathbf{v}_a \pm \mathbf{v}_b \right]^\mathrm{T}.
    \end{equation}
    \item { \emph {Multiplication}}. Multiplication is the multiplication of each item of $ \mathbf {q}_a $ with each item of $ \mathbf {q}_b $ , and finally, the imaginary part is done according to the formula \eqref {eq:quaternionVirtual}:
    \begin{equation}
    \begin{aligned}
    \mathbf{q}_a \mathbf{q}_b &= {s_a}{s_b} - {x_a}{x_b} - {y_a}{y_b} - {z_a}{z_b}\\
    &+ \left( {{s_a}{x_b} + {x_a}{s_b} + {y_a}{z_b} - {z_a}{y_b}} \right)i\\
    &+ \left( {{s_a}{y_b} - {x_a}{z_b} + {y_a}{s_b} + {z_a}{x_b}} \right)j\\
    &+ \left( {{s_a}{z_b} + {x_a}{y_b} - {y_a}{x_b} + {z_a}{s_b}} \right)k.
    \end{aligned}
    \end{equation}
    Although a little complicated, the form is neat and orderly. If written in vector form and using inner and outer product operations, the expression will be more concise:
    \begin{equation}
    \mathbf{q}_a \mathbf{q}_b = \left[ s_a s_b - \mathbf{v}_a^\mathrm{T} \mathbf{v}_b, s_a\mathbf{v}_b + s_b\mathbf{v}_a + \mathbf{v}_a \times \mathbf{v}_b \right]^\mathrm{T}.
    \end{equation}
    Under this multiplication definition, the product of two real quaternion is still real, which is also consistent with the real number multiplication. However, note that due to the existence of the last outer product, quaternion multiplication is usually not commutative unless $ \mathbf {v}_a $ and $ \mathbf {v}_b $ at $ \mathbb {R}^ 3 $ are parallel which means the outer product term is zero.
    
    \item { \emph {Length}. } The length of a quaternion is defined as:
    \begin{equation}
    \| \mathbf{q}_a \| = \sqrt{ s_a^2 + x_a^2 + y_a^2 + z_a^2 }.
    \end{equation}
    It can be verified that the length of the product is the product of the length. This makes the unit quaternion keep unit length when multiplied by another unit quaternion:
    \begin{equation}
    \| \mathbf{q}_a \mathbf{q}_b \| = \|\mathbf{q}_a \| \| \mathbf{q}_b \|.
    \end{equation}
    
    \item { \emph {Conjugate}}. The conjugate of a quaternion is to take the imaginary part as the opposite:
    \begin{equation}
    \mathbf{q}_a ^ * = s_a - x_ai - y_aj - z_ak = [s_a, - \mathbf{v}_a] ^ \mathrm{T}.
    \end{equation}
    We get a real quaternion if the quaternion is multiplied by its conjugate. The real part is the square of its length:
    \begin{equation}
    \mathbf{q}^* \mathbf{q} = \mathbf{q} \mathbf{q}^* = [s_a^2+\mathbf{v}^\mathrm{T} \mathbf{v}, \mathbf{0} ]^\mathrm{T}.
    \end{equation}
    
    \item { \emph{Inverse}}. The inverse of a quaternion is:
    \begin{equation}
    \label{eq:quaternionInverse}
    \mathbf{q} ^ { - 1 } = \mathbf{q} ^ * / \| \mathbf{q} \| ^ 2 .       
    \end{equation}
    According to this definition, the product of the quaternion and its inverse is the real quaternion $ \mathbf {1} $ :
    \begin{equation}
    \mathbf{q} \mathbf{q}^{-1} = \mathbf{q}^{-1} \mathbf{q} = \mathbf{1}.
    \end{equation}
    
    If $ \mathbf{q} $ is a unit quaternion, its inverse and conjugate are the same. So the inverse of the product has properties similar to matrices:
    \begin{equation}
    \left( \mathbf{q}_a \mathbf{q}_b \right)^{-1} = \mathbf{q}_b^{-1} \mathbf{q}_a^{-1}.
    \end{equation}
    
    \item { \emph {Scalar Multiplication}.} Similar to vectors, quaternions can be multiplied by numbers:
    \begin{equation}
    k \mathbf{q} = \left[ ks, k\mathbf{v} \right]^\mathrm{T}.
    \end{equation}
\end{enumerate}

\subsection{Use Quaternion to Represent Rotation}

We can use a quaternion to express the rotation of a point. Suppose a spatial 3D point $ \bm{p} = [x,y,z]^\mathrm{T} \in  \mathbb {R}^3$, and a rotation is specified by a unit quaternion $ \bm{q}$. The 3D point $\bm{p}$ is rotated to become $\bm{p}'$ . If we use matrix, then there is $ \bm{p}'= \bm{R} \bm{p} $ . And if we use quaternion to describe rotation, how do we operate a 3D vector with a quaternion? 

First, we use extends the 3D point to a imaginary quaternion:
\[
\bm{p} = [0, x, y, z]^\mathrm{T} = [0, \bm{v}]^\mathrm{T}. 
\]
We just put the three coordinates into the imaginary part and leave the real part to zero. Then, the rotated point $ \bm {p}' $ can be expressed as such a product:

\begin{equation}\label{eq:rotate-with-quaternion}
\bm{p}' = \bm{q} \bm{p} \bm{q}^{-1}.
\end{equation}

The multiplication here is quaternion multiplication, and the result is also a quaternion. Finally, we take the imaginary part of $ \bm{p}' $ and get the coordinates of the point after the rotation. It can be easily verified (we leave as an exercise here) that the real part of the calculation is 0, so it is a pure imaginary quaternion.

